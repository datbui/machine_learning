A=[2,1;0,1;-2,1;-1,1;1,1];b=[2;0;-2;1;-1];W=pinv(A'*A)*A'*b;E=A*W-b;E'*E;X=[2.49,1; 2.57, 1; 3.41, 1; 1.25, 1; 1.62, 1; 3.83, 1; 11.64, 1; 6.41, 1; 8.34, 1];Y=[147.1; 130.1; 129.9; 113.5; 137.5; 162.3; 207.5; 177.9; 210.3];W=pinv(X'*X)*X'*Y;E=X*W-Y;E'*E;clear all; close all;D=dlmread('/Users/dat/Downloads/linear-regression.train.csv');X_INIT = D(:,1);X = [ones(length(X_INIT), 1), X_INIT] % Add a column of ones to x;Y=D(:,2);theta = zeros(2, 1);     % initialize fitting parametersalpha=0.01; iterations = 100;[t, r, j, e] = gradientDescent(X, Y, theta, alpha, iterations);t = [1:1:100];plot(t, e, 'r');title('Plot the least squares error in each itterations');xlabel('iterations');ylabel('Error');legend('error function');print -dpng 'errors_plot.png'figure 2;plot(X_INIT, Y,'b.', 'MarkerSize', 5);hold on;plot(X_INIT, r,'r');title('Training data with linear regression fit');xlabel('X');ylabel('Y');legend('Training data', 'Linear regression');print -dpng 'optimization_plot.png'figure 3;plot(t, j, 'r');title('Plot the cost function in each itterations');xlabel('iterations');ylabel('Cost');legend('Cost function');print -dpng 'cost_function_plot.png'