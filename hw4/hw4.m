clear all; close all;D=dlmread('hw2_data1.txt');x1=D(:,1);x2=D(:,2);y=D(:,3);alpha=.00001;m = length(y); % number of training examplestheta0 = .1;theta1 = .1;theta2 = .1;iterations=100000;J = zeros(iterations, 1);for i = 1 : iterations  % Gradient calculatoin  %fprintf('Gradient calculatoin  ... \n');    thetatx = theta0+theta1*x1+theta2*x2;  h = 1./(1+exp(-thetatx));  %fprintf('\n\tThetatx %f \n', thetatx);  %fprintf('\n\thypothises %f \n', h);  theta0 = theta0-alpha*sum(h-y);  theta1 = theta1-alpha*sum((h-y).*x1);  theta2 = theta2-alpha*sum((h-y).*x2);  %fprintf('\n\tTheta0 %f Theta1 %f Theta2 %f \n', theta0, theta1, theta2);   % Cost function calculatoin  %fprintf('Cost function calculatoin  ... \n');    thetatx = theta0+theta1*x1+theta2*x2;  h = 1./(1+exp(-thetatx));    %fprintf('\n\tThetatx %f \n', thetatx);  %fprintf('\n\thypothises %f \n', h);    J(i) = -1 * sum(y.*log(h)+(1-y).*log(1-h)); % O HMN   %fprintf('\n\n Cost(%d) %f \n', i, J(i));endfprintf('\n\nMinimal J is %f', min(J));fprintf('\n\ntheta1  is %f theta2  is %f', theta1, theta2);%disp(J);iter = [1:1:iterations];plot(J, 'r');title('Plot the cost function in each iterations');xlabel('iterations');ylabel('Cost');legend('Cost function');xbounds = xlim();set(gca, 'xtick', xbounds(1):10000:xbounds(2));ybounds = ylim();set(gca, 'ytick', ybounds(1):20:ybounds(2));%print -dpng 'cost_function_plot.png';figure 2;yy=-(theta1*x1+theta0)/theta2;% Find Indices of Positive and Negative Examplespos = find(y==1); neg = find(y == 0);% Plot admission resultplot(D(pos,1), D(pos,2), 'r+', D(neg,1), D(neg,2), 'bo', x1, yy, 'g-','linewidth',2);hold on;plot(100, 50, 'm*');title('Plot the decision boundary');xlabel('Exam 1');ylabel('Exam 2');legend('Admitted','Not admitted','Decision boundary', 'Abiturient');%annotation ("arrow", 100, 50);%print -dpng 'decision_boundary.png';% Predictionx1 = 100;x2 = 50;thetatx = theta0+theta1*x1+theta2*x2;prediction = 1./(1+exp(-thetatx));